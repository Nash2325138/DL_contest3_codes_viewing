{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook is using TensorFlow of version: 1.4.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = \"\" # use on GPU\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import _pickle as cPickle\n",
    "\n",
    "print(\"This notebook is using TensorFlow of version: {}\".format(tf.__version__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_allow_growth_session():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = tf.Session(config=config)\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 26900 vocabularies\n"
     ]
    }
   ],
   "source": [
    "vocab = cPickle.load(open('dataset/text/vocab.pkl', 'rb'))\n",
    "print('total {} vocabularies'.format(len(vocab)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cap = pd.read_csv(\n",
    "    'dataset/text/train_enc_cap.csv'\n",
    ")  # a dataframe - 'img_id', 'cpation'\n",
    "enc_map = cPickle.load(open('dataset/text/enc_map.pkl', 'rb'))  # token => id\n",
    "dec_map = cPickle.load(open('dataset/text/dec_map.pkl', 'rb'))  # id => token\n",
    "vocab_size = len(dec_map)\n",
    "\n",
    "\n",
    "def decode(dec_map, ids):\n",
    "    \"\"\"decode IDs back to origin caption string\"\"\"\n",
    "    return ' '.join([dec_map[x] for x in ids])\n",
    "\n",
    "\n",
    "# print('decoding the encoded captions back...\\n')\n",
    "# for idx, row in df_cap.iloc[10:20].iterrows():\n",
    "#     print('{}: {}'.format(idx, decode(dec_map, eval(row['caption']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images for training: 102739\n"
     ]
    }
   ],
   "source": [
    "img_train = cPickle.load(open('dataset/train_img256.pkl', 'rb'))\n",
    "# transform img_dict to dataframe\n",
    "img_train_df = pd.DataFrame(list(img_train.items()), columns=['img_id', 'img'])\n",
    "print('Images for training: {}'.format(img_train_df.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfrecords(df_cap, img_df, filename, num_files=5):\n",
    "    ''' create tfrecords for dataset '''\n",
    "\n",
    "    def _float_feature(value):\n",
    "        return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "    def _int64_feature(value):\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "    num_records_per_file = img_df.shape[0] // num_files\n",
    "\n",
    "    total_count = 0\n",
    "\n",
    "    print(\"create training dataset....\")\n",
    "    for i in range(num_files):\n",
    "        # tfrecord writer: write record into files\n",
    "        count = 0\n",
    "        writer = tf.python_io.TFRecordWriter(filename + '-' + str(i + 1) +\n",
    "                                             '.tfrecord')\n",
    "\n",
    "        # put remaining records in last file\n",
    "        st = i * num_records_per_file  # start point (inclusive)\n",
    "        ed = (i + 1) * num_records_per_file if i != num_files - 1 else img_df.shape[0]  # end point (exclusive)\n",
    "\n",
    "        for idx, row in img_df.iloc[st:ed].iterrows():\n",
    "\n",
    "            img_representation = row['img']  # img representation in 256-d array format\n",
    "\n",
    "            # each image has some captions describing it.\n",
    "            for _, inner_row in df_cap[df_cap['img_id'] == row['img_id']].iterrows():\n",
    "                caption = eval(\n",
    "                    inner_row['caption']\n",
    "                )  # caption in different sequence length list format\n",
    "\n",
    "                # construct 'example' object containing 'img', 'caption'\n",
    "                example = tf.train.Example(features=tf.train.Features(\n",
    "                    feature={\n",
    "                        'img': _float_feature(img_representation),\n",
    "                        'caption': _int64_feature(caption)\n",
    "                    }\n",
    "                ))\n",
    "\n",
    "                count += 1\n",
    "                writer.write(example.SerializeToString())\n",
    "        print(\"create {}-{}.tfrecord -- contains {} records\".format(\n",
    "            filename, str(i + 1), count))\n",
    "        total_count += count\n",
    "        writer.close()\n",
    "    print(\"Total records: {}\".format(total_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment next line to create tfrecords file\n",
    "# create_tfrecords(df_cap, img_train_df, 'dataset/tfrecord/train', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train records in each training file: 513969\n"
     ]
    }
   ],
   "source": [
    "training_filenames = [\n",
    "    \"dataset/tfrecord/train-1.tfrecord\",\n",
    "    \"dataset/tfrecord/train-2.tfrecord\",\n",
    "    \"dataset/tfrecord/train-3.tfrecord\", \"dataset/tfrecord/train-4.tfrecord\",\n",
    "    \"dataset/tfrecord/train-5.tfrecord\", \"dataset/tfrecord/train-6.tfrecord\",\n",
    "    \"dataset/tfrecord/train-7.tfrecord\", \"dataset/tfrecord/train-8.tfrecord\",\n",
    "    \"dataset/tfrecord/train-9.tfrecord\", \"dataset/tfrecord/train-10.tfrecord\"\n",
    "]\n",
    "\n",
    "\n",
    "# get the number of records in training files\n",
    "def get_num_records(files):\n",
    "    count = 0\n",
    "    for fn in files:\n",
    "        for record in tf.python_io.tf_record_iterator(fn):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "num_train_records = get_num_records(training_filenames)\n",
    "print('Number of train records in each training file: {}'.format(\n",
    "    num_train_records))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_parser(record):\n",
    "    ''' parse record from .tfrecord file and create training record\n",
    "    \n",
    "    :args \n",
    "      record - each record extracted from .tfrecord\n",
    "\n",
    "    :return\n",
    "      a dictionary contains {\n",
    "          'img': image array extracted from vgg16 (256-dim) (Tensor),\n",
    "          'input_seq': a list of word id\n",
    "                    which describes input caption sequence (Tensor),\n",
    "          'output_seq': a list of word id\n",
    "                    which describes output caption sequence (Tensor),\n",
    "          'mask': a list of one which describe\n",
    "                    the length of input caption sequence (Tensor)\n",
    "      }\n",
    "    '''\n",
    "\n",
    "    keys_to_features = {\n",
    "        \"img\": tf.FixedLenFeature([256], dtype=tf.float32),\n",
    "        \"caption\": tf.VarLenFeature(dtype=tf.int64)\n",
    "    }\n",
    "\n",
    "    # features contains - 'img', 'caption'\n",
    "    features = tf.parse_single_example(record, features=keys_to_features)\n",
    "\n",
    "    img = features['img']  # tensor\n",
    "    caption = features[\n",
    "        'caption'].values  # tensor (features['caption'] - sparse_tensor)\n",
    "    caption = tf.cast(caption, tf.int32)\n",
    "\n",
    "    # create input and output sequence for each training example\n",
    "    # e.g. caption :   [0 2 5 7 9 1]\n",
    "    #      input_seq:  [0 2 5 7 9]\n",
    "    #      output_seq: [2 5 7 9 1]\n",
    "    #      mask:       [1 1 1 1 1]\n",
    "    caption_len = tf.shape(caption)[0]\n",
    "    input_len = tf.expand_dims(tf.subtract(caption_len, 1), 0)\n",
    "\n",
    "    input_seq = tf.slice(caption, [0], input_len)\n",
    "    output_seq = tf.slice(caption, [1], input_len)\n",
    "    mask = tf.ones(input_len, dtype=tf.int32)\n",
    "\n",
    "    records = {\n",
    "        'img': img,\n",
    "        'input_seq': input_seq,\n",
    "        'output_seq': output_seq,\n",
    "        'mask': mask\n",
    "    }\n",
    "\n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfrecord_iterator(filenames, batch_size, record_parser):\n",
    "    ''' create iterator to eat tfrecord dataset \n",
    "    \n",
    "    :args\n",
    "        filenames     - a list of filenames (string)\n",
    "        batch_size    - batch size (positive int)\n",
    "        record_parser - a parser that read tfrecord\n",
    "                        and create example record (function)\n",
    "\n",
    "    :return \n",
    "        iterator      - an Iterator providing a way\n",
    "                        to extract elements from the created dataset.\n",
    "        output_types  - the output types of the created dataset.\n",
    "        output_shapes - the output shapes of the created dataset.\n",
    "    '''\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "    dataset = dataset.map(record_parser, num_parallel_calls=16)\n",
    "\n",
    "    # padded into equal length in each batch\n",
    "    dataset = dataset.padded_batch(\n",
    "        batch_size=batch_size,\n",
    "        padded_shapes={\n",
    "            'img': [None],\n",
    "            'input_seq': [None],\n",
    "            'output_seq': [None],\n",
    "            'mask': [None]\n",
    "        },\n",
    "        padding_values={\n",
    "            'img': 1.0,  # needless, for completeness\n",
    "            'input_seq': 1,  # padding input sequence in this batch\n",
    "            'output_seq': 1,  # padding output sequence in this batch\n",
    "            'mask': 0  # padding 0 means no words in this position\n",
    "        })\n",
    "\n",
    "    dataset = dataset.repeat()  # repeat dataset infinitely\n",
    "    dataset = dataset.shuffle(batch_size)  # shuffle the dataset\n",
    "\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    output_types = dataset.output_types\n",
    "    output_shapes = dataset.output_shapes\n",
    "\n",
    "    return iterator, output_types, output_shapes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionModel(object):\n",
    "    ''' simple image caption model '''\n",
    "\n",
    "    def __init__(self, hparams, mode):\n",
    "        self.hps = hparams\n",
    "        self.mode = mode\n",
    "\n",
    "    def _build_inputs(self):\n",
    "        if self.mode == 'train':\n",
    "            self.filenames = tf.placeholder(\n",
    "                tf.string, shape=[None], name='filenames')\n",
    "            self.training_iterator, types, shapes = tfrecord_iterator(\n",
    "                self.filenames, self.hps.batch_size, training_parser)\n",
    "\n",
    "            self.handle = tf.placeholder(tf.string, shape=[], name='handle')\n",
    "            iterator = tf.data.Iterator.from_string_handle(\n",
    "                self.handle, types, shapes)\n",
    "            records = iterator.get_next()\n",
    "\n",
    "            image_embed = records['img']\n",
    "            image_embed.set_shape([None, self.hps.image_embedding_size])\n",
    "            input_seq = records['input_seq']\n",
    "            target_seq = records['output_seq']\n",
    "            input_mask = records['mask']\n",
    "\n",
    "        else:\n",
    "            image_embed = tf.placeholder(\n",
    "                tf.float32,\n",
    "                shape=[None, self.hps.image_embedding_size],\n",
    "                name='image_embed')\n",
    "            input_feed = tf.placeholder(\n",
    "                tf.int32, shape=[None], name='input_feed')\n",
    "\n",
    "            input_seq = tf.expand_dims(input_feed, axis=1)\n",
    "            # in inference step, only use image_embed\n",
    "            # and input_seq (the first start word)\n",
    "            target_seq = None\n",
    "            input_mask = None\n",
    "\n",
    "        self.image_embed = image_embed\n",
    "        self.input_seq = input_seq\n",
    "        self.target_seq = target_seq\n",
    "        self.input_mask = input_mask\n",
    "\n",
    "    def _build_seq_embeddings(self):\n",
    "        with tf.variable_scope('seq_embedding'), tf.device('/cpu:0'):\n",
    "            embedding_matrix = tf.get_variable(\n",
    "                name='embedding_matrix',\n",
    "                shape=[self.hps.vocab_size, self.hps.word_embedding_size],\n",
    "                initializer=tf.random_uniform_initializer(minval=-1, maxval=1))\n",
    "            # [batch_size, padded_length, embedding_size]\n",
    "            seq_embeddings = tf.nn.embedding_lookup(embedding_matrix,\n",
    "                                                    self.input_seq)\n",
    "\n",
    "        self.seq_embeddings = seq_embeddings\n",
    "\n",
    "    def _build_model(self):\n",
    "        # create rnn cell, you can choose different cell,\n",
    "        # even stack into multi-layer rnn\n",
    "        rnn_cell = tf.nn.rnn_cell.BasicLSTMCell(\n",
    "            num_units=self.hps.rnn_units, state_is_tuple=True)\n",
    "\n",
    "        # when training, add dropout to regularize.\n",
    "        if self.mode == 'train':\n",
    "            rnn_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                rnn_cell,\n",
    "                input_keep_prob=self.hps.drop_keep_prob,\n",
    "                output_keep_prob=self.hps.drop_keep_prob)\n",
    "\n",
    "        # run rnn\n",
    "        with tf.variable_scope(\n",
    "                'rnn_scope',\n",
    "                initializer=tf.random_uniform_initializer(\n",
    "                    minval=-1, maxval=1)) as rnn_scope:\n",
    "\n",
    "            # feed the image embeddings to set the initial rnn state.\n",
    "            zero_state = rnn_cell.zero_state(\n",
    "                batch_size=tf.shape(self.image_embed)[0], dtype=tf.float32)\n",
    "            _, initial_state = rnn_cell(self.image_embed, zero_state)\n",
    "\n",
    "            rnn_scope.reuse_variables()\n",
    "\n",
    "            if self.mode == 'train':\n",
    "                sequence_length = tf.reduce_sum(self.input_mask, 1)\n",
    "                outputs, _ = tf.nn.dynamic_rnn(\n",
    "                    cell=rnn_cell,\n",
    "                    inputs=self.seq_embeddings,\n",
    "                    sequence_length=sequence_length,\n",
    "                    initial_state=initial_state,\n",
    "                    dtype=tf.float32,\n",
    "                    scope=rnn_scope)\n",
    "            else:\n",
    "                # in inference mode,\n",
    "                #  use concatenated states for convenient feeding and fetching.\n",
    "                initial_state = tf.concat(\n",
    "                    values=initial_state, axis=1, name='initial_state')\n",
    "\n",
    "                state_feed = tf.placeholder(\n",
    "                    tf.float32,\n",
    "                    shape=[None, sum(rnn_cell.state_size)],\n",
    "                    name='state_feed')\n",
    "                state_tuple = tf.split(\n",
    "                    value=state_feed, num_or_size_splits=2, axis=1)\n",
    "\n",
    "                # run a single rnn step\n",
    "                outputs, state = rnn_cell(\n",
    "                    inputs=tf.squeeze(self.seq_embeddings, axis=[1]),\n",
    "                    state=state_tuple)\n",
    "\n",
    "                # concatenate the resulting state.\n",
    "                final_state = tf.concat(\n",
    "                    values=state, axis=1, name='final_state')\n",
    "\n",
    "        # stack rnn output vertically\n",
    "        # [sequence_len * batch_size, rnn_output_size]\n",
    "        rnn_outputs = tf.reshape(outputs, [-1, rnn_cell.output_size])\n",
    "\n",
    "        # get logits after transforming from dense layer\n",
    "        with tf.variable_scope(\"logits\") as logits_scope:\n",
    "            rnn_out = {\n",
    "                'weights':\n",
    "                tf.Variable(\n",
    "                    tf.random_normal(\n",
    "                        shape=[self.hps.rnn_units, self.hps.vocab_size],\n",
    "                        mean=0.0,\n",
    "                        stddev=0.1,\n",
    "                        dtype=tf.float32)),\n",
    "                'bias':\n",
    "                tf.Variable(tf.zeros(shape=[self.hps.vocab_size]))\n",
    "            }\n",
    "\n",
    "            # logits [batch_size*seq_len, vocab_size]\n",
    "            logits = tf.add(\n",
    "                tf.matmul(rnn_outputs, rnn_out['weights']), rnn_out['bias'])\n",
    "\n",
    "        with tf.name_scope('optimize') as optimize_scope:\n",
    "            if self.mode == 'train':\n",
    "                targets = tf.reshape(self.target_seq,\n",
    "                                     [-1])  # flatten to 1-d tensor\n",
    "                indicator = tf.cast(\n",
    "                    tf.reshape(self.input_mask, [-1]), tf.float32)\n",
    "\n",
    "                # loss function\n",
    "                losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    labels=targets, logits=logits)\n",
    "                batch_loss = tf.div(\n",
    "                    tf.reduce_sum(tf.multiply(losses, indicator)),\n",
    "                    tf.reduce_sum(indicator),\n",
    "                    name='batch_loss')\n",
    "\n",
    "                # add some regularizer or tricks to train well\n",
    "                self.total_loss = batch_loss\n",
    "\n",
    "                # save checkpoint\n",
    "                self.global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "                # create optimizer\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate=self.hps.lr)\n",
    "                self.train_op = optimizer.minimize(\n",
    "                    self.total_loss, global_step=self.global_step)\n",
    "\n",
    "            else:\n",
    "                pred_softmax = tf.nn.softmax(logits, name='softmax')\n",
    "                prediction = tf.argmax(pred_softmax, axis=1, name='prediction')\n",
    "\n",
    "    def build(self):\n",
    "        self._build_inputs()\n",
    "        self._build_seq_embeddings()\n",
    "        self._build_model()\n",
    "\n",
    "    def train(self, training_filenames, num_train_records):\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        with get_allow_growth_session() as sess:\n",
    "            ckpt = tf.train.get_checkpoint_state(self.hps.ckpt_dir)\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                # if checkpoint exists\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                # assume the name of checkpoint is like '.../model.ckpt-1000'\n",
    "                gs = int(\n",
    "                    ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1])\n",
    "                sess.run(tf.assign(self.global_step, gs))\n",
    "            else:\n",
    "                # no checkpoint\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            training_handle = sess.run(self.training_iterator.string_handle())\n",
    "            sess.run(\n",
    "                self.training_iterator.initializer,\n",
    "                feed_dict={self.filenames: training_filenames})\n",
    "\n",
    "            num_batch_per_epoch_train = num_train_records // self.hps.batch_size\n",
    "\n",
    "            loss = []\n",
    "            for epoch in range(self.hps.training_epochs):\n",
    "                _loss = []\n",
    "                for i in range(num_batch_per_epoch_train):\n",
    "                    train_loss_batch, _ = sess.run(\n",
    "                        [self.total_loss, self.train_op],\n",
    "                        feed_dict={self.handle: training_handle})\n",
    "                    _loss.append(train_loss_batch)\n",
    "                    if (i % 1000 == 0):\n",
    "                        print(\"minibatch training loss: {:.4f}\".format(\n",
    "                            train_loss_batch))\n",
    "                loss_this_epoch = np.sum(_loss)\n",
    "                gs = self.global_step.eval()\n",
    "                print('Epoch {:2d} - train loss: {:.4f}'.format(\n",
    "                    int(gs / num_batch_per_epoch_train), loss_this_epoch))\n",
    "                loss.append(loss_this_epoch)\n",
    "                saver.save(\n",
    "                    sess, self.hps.ckpt_dir + 'model.ckpt', global_step=gs)\n",
    "                print(\"save checkpoint in {}\".format(self.hps.ckpt_dir +\n",
    "                                                     'model.ckpt-' + str(gs)))\n",
    "\n",
    "            print('Done')\n",
    "\n",
    "    def beam_search(self, sess, rnn_state, prev_word, log_beam_prob, beam_size=3):\n",
    "        probs, next_state = sess.run(\n",
    "            fetches=['optimize/softmax:0', 'rnn_scope/final_state:0'],\n",
    "            feed_dict={\n",
    "                'input_feed:0': [prev_word],\n",
    "                'rnn_scope/state_feed:0': rnn_state\n",
    "            })\n",
    "        probs = probs[0]\n",
    "        probs_logsum = np.log(probs + 0.00001) + log_beam_prob\n",
    "        indices = np.argsort(probs_logsum)[::-1][0:beam_size]\n",
    "        best_probs = []\n",
    "        for idx in indices:\n",
    "            best_probs.append(probs_logsum[idx])\n",
    "        #  best_probs, indices = sess.run(tf.nn.top_k(probs_logsum, k=beam_size))\n",
    "        #  best_probs, indices = best_probs[0], indices[0]\n",
    "        next_beam_probs = []\n",
    "        next_words = []\n",
    "        for i in range(beam_size):\n",
    "            next_beam_probs.append(best_probs[i])\n",
    "            next_words.append(indices[i])\n",
    "        return next_state, next_words, next_beam_probs\n",
    "\n",
    "    def inference(self, sess, img_embed, enc_map, dec_map):\n",
    "        # get <start> and <end> word id\n",
    "        st, ed = enc_map['<ST>'], enc_map['<ED>']\n",
    "\n",
    "        caption_id = []\n",
    "        # feed into input_feed\n",
    "        start_word_feed = [st]\n",
    "\n",
    "        # feed image_embed into initial state\n",
    "        initial_state = sess.run(\n",
    "                fetches='rnn_scope/initial_state:0',\n",
    "                feed_dict={'image_embed:0': img_embed})\n",
    "\n",
    "        # get the first word and its state\n",
    "        nxt_word, this_state = sess.run(\n",
    "                fetches=['optimize/prediction:0', 'rnn_scope/final_state:0'],\n",
    "                feed_dict={\n",
    "                        'input_feed:0': start_word_feed,\n",
    "                        'rnn_scope/state_feed:0': initial_state\n",
    "                })\n",
    "\n",
    "        caption_id.append(int(nxt_word))\n",
    "\n",
    "        for i in range(self.hps.max_caption_len - 1):\n",
    "            nxt_word, this_state = sess.run(\n",
    "                    fetches=['optimize/prediction:0', 'rnn_scope/final_state:0'],\n",
    "                    feed_dict={\n",
    "                            'input_feed:0': nxt_word,\n",
    "                            'rnn_scope/state_feed:0': this_state\n",
    "                    })\n",
    "            caption_id.append(int(nxt_word))\n",
    "\n",
    "        caption = [\n",
    "                dec_map[x]\n",
    "                for x in caption_id[:None\n",
    "                    if ed not in caption_id else caption_id.index(ed)]\n",
    "        ]\n",
    "\n",
    "        return ' '.join(caption)\n",
    "\n",
    "    def beam_inference(self, sess, img_embed, enc_map, dec_map):\n",
    "        # get <start> and <end> word id\n",
    "        st, ed = enc_map['<ST>'], enc_map['<ED>']\n",
    "\n",
    "        # feed image_embed into initial state\n",
    "        initial_state = sess.run(\n",
    "            fetches='rnn_scope/initial_state:0',\n",
    "            feed_dict={'image_embed:0': img_embed})\n",
    "\n",
    "        # feed into input_feed\n",
    "        start_word_feed = st\n",
    "        # beam search\n",
    "        beam_size = 3\n",
    "\n",
    "        state, words, probs = self.beam_search(sess, initial_state, start_word_feed, [0], beam_size)\n",
    "        states = [state for i in range(beam_size)]\n",
    "\n",
    "        captions = [[] for i in range(beam_size)]\n",
    "        for i in range(beam_size):\n",
    "            captions[i].append(words[i])\n",
    "\n",
    "        for i in range(self.hps.max_caption_len - 1):\n",
    "            all_beam_states = []\n",
    "            all_beam_words = []\n",
    "            all_beam_probs = []\n",
    "            for j in range(beam_size):\n",
    "                nstate, nwords, nprobs = self.beam_search(sess, states[j], words[j], probs[j], beam_size)\n",
    "                for _ in range(beam_size):\n",
    "                    all_beam_states.append(nstate)\n",
    "                all_beam_words.extend(nwords)\n",
    "                all_beam_probs.extend(nprobs)\n",
    "            indices = (np.argsort(all_beam_probs)[::-1])[0:beam_size]\n",
    "            new_captions = [[] for i in range(beam_size)]\n",
    "            for j, index in enumerate(indices):\n",
    "                cap_id = index // beam_size\n",
    "                new_captions[j].extend(captions[cap_id])\n",
    "                new_captions[j].append(all_beam_words[index])\n",
    "                states[j] = all_beam_states[index]\n",
    "                words[j] = all_beam_words[index]\n",
    "                probs[j] = all_beam_probs[index]\n",
    "            captions = new_captions\n",
    "\n",
    "        caption_sentences = []\n",
    "        for caption in captions:\n",
    "            word_caption = [dec_map[x]\n",
    "                            for x in caption[:None if ed not in caption else\n",
    "                                caption.index(ed)]]\n",
    "            caption_sentences.append(' '.join(word_caption))\n",
    "\n",
    "        return caption_sentences[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hparams():\n",
    "    hparams = tf.contrib.training.HParams(\n",
    "        vocab_size=vocab_size,\n",
    "        batch_size=64,\n",
    "        rnn_units=256,\n",
    "        image_embedding_size=256,\n",
    "        word_embedding_size=256,\n",
    "        drop_keep_prob=0.7,\n",
    "        lr=1e-3,\n",
    "        training_epochs=1,\n",
    "        max_caption_len=15,\n",
    "        ckpt_dir='model_ckpt/sample')\n",
    "    return hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = get_hparams()\n",
    "# rnn_units should be the same with image_embedding_size in our model\n",
    "assert (hparams.word_embedding_size == hparams.image_embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create model\n",
    "# tf.reset_default_graph()\n",
    "# model = ImageCaptionModel(hparams, mode='train')\n",
    "# model.build()\n",
    "# start training\n",
    "# model.train(training_filenames, num_train_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_captions(model, enc_map, dec_map, img_test, max_len=15):\n",
    "    img_ids, caps = [], []\n",
    "\n",
    "    with get_allow_growth_session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        # restore variables from disk.\n",
    "        ckpt = tf.train.get_checkpoint_state(hparams.ckpt_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, tf.train.latest_checkpoint(hparams.ckpt_dir))\n",
    "\n",
    "            counter = 0\n",
    "            total = len(list(img_test.items()))\n",
    "            for img_id, img in img_test.items():\n",
    "                if counter % 500 == 0:\n",
    "                    print('Processing {}/{}'.format(counter, total))\n",
    "                counter += 1\n",
    "                img_ids.append(img_id)\n",
    "                img = np.expand_dims(img, axis=0)\n",
    "                caps.append(model.beam_inference(sess, img, enc_map, dec_map))\n",
    "\n",
    "        else:\n",
    "            print(\"No checkpoint found.\")\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'img_id': img_ids,\n",
    "        'caption': caps\n",
    "    }).set_index(['img_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model_ckpt/sample/model.ckpt-662160\n",
      "Processing 0/20548\n",
      "Processing 500/20548\n",
      "Processing 1000/20548\n",
      "Processing 1500/20548\n",
      "Processing 2000/20548\n",
      "Processing 2500/20548\n",
      "Processing 3000/20548\n",
      "Processing 3500/20548\n",
      "Processing 4000/20548\n",
      "Processing 4500/20548\n",
      "Processing 5000/20548\n",
      "Processing 5500/20548\n",
      "Processing 6000/20548\n",
      "Processing 6500/20548\n",
      "Processing 7000/20548\n",
      "Processing 7500/20548\n",
      "Processing 8000/20548\n",
      "Processing 8500/20548\n",
      "Processing 9000/20548\n",
      "Processing 9500/20548\n",
      "Processing 10000/20548\n",
      "Processing 10500/20548\n",
      "Processing 11000/20548\n",
      "Processing 11500/20548\n",
      "Processing 12000/20548\n",
      "Processing 12500/20548\n",
      "Processing 13000/20548\n",
      "Processing 13500/20548\n",
      "Processing 14000/20548\n",
      "Processing 14500/20548\n",
      "Processing 15000/20548\n",
      "Processing 15500/20548\n",
      "Processing 16000/20548\n",
      "Processing 16500/20548\n",
      "Processing 17000/20548\n",
      "Processing 17500/20548\n",
      "Processing 18000/20548\n",
      "Processing 18500/20548\n",
      "Processing 19000/20548\n",
      "Processing 19500/20548\n",
      "Processing 20000/20548\n",
      "Processing 20500/20548\n",
      "CPU times: user 36min 22s, sys: 2min 45s, total: 39min 7s\n",
      "Wall time: 38min 23s\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "tf.reset_default_graph()\n",
    "model = ImageCaptionModel(hparams, mode='inference')\n",
    "model.build()\n",
    "\n",
    "# load test image  size=20548\n",
    "img_test = cPickle.load(open('dataset/test_img256.pkl', 'rb'))\n",
    "\n",
    "# generate caption to csv file\n",
    "%time df_predict = generate_captions(model, enc_map, dec_map, img_test)\n",
    "df_predict.to_csv('generated/sample_470_epoch_beam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
